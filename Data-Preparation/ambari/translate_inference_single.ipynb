{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tritonclient[http]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is server ready - True\n",
      "\n",
      "When I was young, I used to go to the park every day.\n",
      "ನಾನು ಚಿಕ್ಕವಳಿದ್ದಾಗ, ಪ್ರತಿದಿನ ಉದ್ಯಾನವನಕ್ಕೆ ಹೋಗುತ್ತಿದ್ದೆ.\n",
      "\n",
      "He has many old books, which he inherited from his ancestors.\n",
      "ಅವರು ತಮ್ಮ ಪೂರ್ವಜರಿಂದ ಆನುವಂಶಿಕವಾಗಿ ಪಡೆದ ಅನೇಕ ಹಳೆಯ ಪುಸ್ತಕಗಳನ್ನು ಹೊಂದಿದ್ದಾರೆ.\n",
      "\n",
      "I can't figure out how to solve my problem.\n",
      "ನನ್ನ ಸಮಸ್ಯೆಯನ್ನು ಹೇಗೆ ಪರಿಹರಿಸಬೇಕೆಂದು ನನಗೆ ಅರ್ಥವಾಗುತ್ತಿಲ್ಲ.\n",
      "\n",
      "She is very hardworking and intelligent, which is why she got all the good marks.\n",
      "ಅವಳು ತುಂಬಾ ಕಷ್ಟಪಟ್ಟು ದುಡಿಯುವವಳು ಮತ್ತು ಬುದ್ಧಿವಂತಳು, ಅದಕ್ಕಾಗಿಯೇ ಅವಳು ಎಲ್ಲಾ ಉತ್ತಮ ಅಂಕಗಳನ್ನು ಪಡೆದಳು.\n",
      "\n",
      "We watched a new movie last week, which was very inspiring.\n",
      "ನಾವು ಕಳೆದ ವಾರ ಹೊಸ ಚಲನಚಿತ್ರವೊಂದನ್ನು ನೋಡಿದೆವು, ಅದು ಬಹಳ ಸ್ಪೂರ್ತಿದಾಯಕವಾಗಿತ್ತು.\n",
      "\n",
      "If you had met me at that time, we would have gone out to eat.\n",
      "ಆ ಸಮಯದಲ್ಲಿ ನೀವು ನನ್ನನ್ನು ಭೇಟಿಯಾಗಿದ್ದರೆ, ನಾವು ತಿನ್ನಲು ಹೊರಗೆ ಹೋಗುತ್ತಿದ್ದೆವು.\n",
      "\n",
      "She went to the market with her sister to buy a new sari.\n",
      "ಆಕೆ ತನ್ನ ಸಹೋದರಿಯೊಂದಿಗೆ ಹೊಸ ಸೀರೆಯನ್ನು ಖರೀದಿಸಲು ಮಾರುಕಟ್ಟೆಗೆ ಹೋದಳು.\n",
      "\n",
      "Raj told me that he is going to his grandmother's house next month.\n",
      "ಮುಂದಿನ ತಿಂಗಳು ತನ್ನ ಅಜ್ಜಿಯ ಮನೆಗೆ ಹೋಗುತ್ತಿದ್ದೇನೆ ಎಂದು ರಾಜ್ ನನಗೆ ಹೇಳಿದನು.\n",
      "\n",
      "All the kids were having fun at the party and were eating lots of sweets.\n",
      "ಎಲ್ಲಾ ಮಕ್ಕಳು ಪಾರ್ಟಿಯಲ್ಲಿ ಮೋಜು ಮಾಡುತ್ತಿದ್ದರು ಮತ್ತು ಸಾಕಷ್ಟು ಸಿಹಿತಿಂಡಿಗಳನ್ನು ತಿನ್ನುತ್ತಿದ್ದರು.\n",
      "\n",
      "My friend has invited me to his birthday party, and I will give him a gift.\n",
      "ನನ್ನ ಸ್ನೇಹಿತ ತನ್ನ ಹುಟ್ಟುಹಬ್ಬದ ಸಂತೋಷಕೂಟಕ್ಕೆ ನನ್ನನ್ನು ಆಹ್ವಾನಿಸಿದ್ದಾನೆ ಮತ್ತು ನಾನು ಅವನಿಗೆ ಉಡುಗೊರೆಯನ್ನು ನೀಡುತ್ತೇನೆ.\n"
     ]
    }
   ],
   "source": [
    "import tritonclient.http as http_client\n",
    "from tritonclient.utils import *\n",
    "import numpy as np\n",
    "\n",
    "ENABLE_SSL = False\n",
    "ENDPOINT_URL = '52.151.255.217:8000'\n",
    "HTTP_HEADERS = {\"Authorization\": \"Bearer __PASTE_KEY_HERE__\"}\n",
    "\n",
    "# Connect to the server\n",
    "if ENABLE_SSL:\n",
    "    import gevent.ssl\n",
    "    triton_http_client = http_client.InferenceServerClient(\n",
    "        url=ENDPOINT_URL, verbose=False,\n",
    "        ssl=True, ssl_context_factory=gevent.ssl._create_default_https_context,\n",
    "    )\n",
    "else:\n",
    "    triton_http_client = http_client.InferenceServerClient(\n",
    "        url=ENDPOINT_URL, verbose=False,\n",
    "    )\n",
    "\n",
    "print(\"Is server ready - {}\".format(triton_http_client.is_server_ready(headers=HTTP_HEADERS)))\n",
    "\n",
    "def get_string_tensor(string_values, tensor_name):\n",
    "    string_obj = np.array(string_values, dtype=\"object\")\n",
    "    input_obj = http_client.InferInput(tensor_name, string_obj.shape, np_to_triton_dtype(string_obj.dtype))\n",
    "    input_obj.set_data_from_numpy(string_obj)\n",
    "    return input_obj\n",
    "\n",
    "def get_translation_input_for_triton(texts: list, src_lang: str, tgt_lang: str):\n",
    "    return [\n",
    "        get_string_tensor([[text] for text in texts], \"INPUT_TEXT\"),\n",
    "        get_string_tensor([[src_lang]] * len(texts), \"INPUT_LANGUAGE_ID\"),\n",
    "        get_string_tensor([[tgt_lang]] * len(texts), \"OUTPUT_LANGUAGE_ID\"),\n",
    "    ]\n",
    "\n",
    "# Prepare input and output tensors\n",
    "input_sentences = [\n",
    "    \"When I was young, I used to go to the park every day.\",\n",
    "    \"He has many old books, which he inherited from his ancestors.\",\n",
    "    \"I can't figure out how to solve my problem.\",\n",
    "    \"She is very hardworking and intelligent, which is why she got all the good marks.\",\n",
    "    \"We watched a new movie last week, which was very inspiring.\",\n",
    "    \"If you had met me at that time, we would have gone out to eat.\",\n",
    "    \"She went to the market with her sister to buy a new sari.\",\n",
    "    \"Raj told me that he is going to his grandmother's house next month.\",\n",
    "    \"All the kids were having fun at the party and were eating lots of sweets.\",\n",
    "    \"My friend has invited me to his birthday party, and I will give him a gift.\",\n",
    "]\n",
    "inputs = get_translation_input_for_triton(input_sentences, \"en\", \"kn\")\n",
    "output0 = http_client.InferRequestedOutput(\"OUTPUT_TEXT\")\n",
    "\n",
    "# Send request\n",
    "response = triton_http_client.infer(\n",
    "    \"nmt\",\n",
    "    model_version='1',\n",
    "    inputs=inputs,\n",
    "    outputs=[output0],\n",
    "    headers=HTTP_HEADERS,\n",
    ")#.get_response()\n",
    "\n",
    "# Decode the response\n",
    "output_batch = response.as_numpy('OUTPUT_TEXT').tolist()\n",
    "for input_sentence, translation in zip(input_sentences, output_batch):\n",
    "    print()\n",
    "    print(input_sentence)\n",
    "    print(translation[0].decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"CognitiveLab/English_Instruction_Combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'output'],\n",
       "        num_rows: 417314\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is server ready - True\n"
     ]
    }
   ],
   "source": [
    "import tritonclient.http as http_client\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "ENABLE_SSL = False\n",
    "ENDPOINT_URL = '52.151.255.217:8000'\n",
    "HTTP_HEADERS = {\"Authorization\": \"Bearer __PASTE_KEY_HERE__\"}\n",
    "\n",
    "# Connect to the server\n",
    "if ENABLE_SSL:\n",
    "    import gevent.ssl\n",
    "    triton_http_client = http_client.InferenceServerClient(\n",
    "        url=ENDPOINT_URL, verbose=False,\n",
    "        ssl=True, ssl_context_factory=gevent.ssl._create_default_https_context,\n",
    "    )\n",
    "else:\n",
    "    triton_http_client = http_client.InferenceServerClient(\n",
    "        url=ENDPOINT_URL, verbose=False,\n",
    "    )\n",
    "\n",
    "print(\"Is server ready - {}\".format(triton_http_client.is_server_ready(headers=HTTP_HEADERS)))\n",
    "\n",
    "def get_string_tensor(string_values, tensor_name):\n",
    "    string_obj = np.array(string_values, dtype=\"object\")\n",
    "    input_obj = http_client.InferInput(tensor_name, string_obj.shape, np_to_triton_dtype(string_obj.dtype))\n",
    "    input_obj.set_data_from_numpy(string_obj)\n",
    "    return input_obj\n",
    "\n",
    "def get_translation_input_for_triton(texts: list, src_lang: str, tgt_lang: str):\n",
    "    return [\n",
    "        get_string_tensor([[text] for text in texts], \"INPUT_TEXT\"),\n",
    "        get_string_tensor([[src_lang]] * len(texts), \"INPUT_LANGUAGE_ID\"),\n",
    "        get_string_tensor([[tgt_lang]] * len(texts), \"OUTPUT_LANGUAGE_ID\"),\n",
    "    ]\n",
    "\n",
    "def translate_to_kannada(texts):\n",
    "    inputs = get_translation_input_for_triton(texts, \"en\", \"kn\")\n",
    "    output0 = http_client.InferRequestedOutput(\"OUTPUT_TEXT\")\n",
    "\n",
    "    # Send request\n",
    "    response = triton_http_client.infer(\n",
    "        \"nmt\",\n",
    "        model_version='1',\n",
    "        inputs=inputs,\n",
    "        outputs=[output0],\n",
    "        headers=HTTP_HEADERS,\n",
    "    )\n",
    "\n",
    "    # Decode the response\n",
    "    output_batch = response.as_numpy('OUTPUT_TEXT').tolist()\n",
    "    translations = [translation[0].decode(\"utf-8\") for translation in output_batch]\n",
    "    \n",
    "    return translations\n",
    "\n",
    "def save_to_csv(original_instructions, original_outputs, translated_instructions, translated_outputs, file_path):\n",
    "    data = {\n",
    "        \"original_instruction\": original_instructions,\n",
    "        \"original_output\": original_outputs,\n",
    "        \"translated_instruction\": translated_instructions,\n",
    "        \"translated_output\": translated_outputs\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "def translate_dataset_to_kannada(dataset,output_folder=\"final_translate_data\"):\n",
    "    batch_size = 512  # Adjust the batch size based on your requirements\n",
    "    num_examples = len(dataset[\"instruction\"])\n",
    "\n",
    "    translated_instructions = []\n",
    "    translated_outputs = []\n",
    "\n",
    "    for start_idx in tqdm(range(0, num_examples, batch_size), desc=\"Translating Dataset\"):\n",
    "        end_idx = min(start_idx + batch_size, num_examples)\n",
    "        batch_instructions = dataset[\"instruction\"][start_idx:end_idx]\n",
    "        batch_outputs = dataset[\"output\"][start_idx:end_idx]\n",
    "\n",
    "        translated_batch_instructions = translate_to_kannada(batch_instructions)\n",
    "        translated_batch_outputs = translate_to_kannada(batch_outputs)\n",
    "\n",
    "        translated_instructions.extend(translated_batch_instructions)\n",
    "        translated_outputs.extend(translated_batch_outputs)\n",
    "         # Save to CSV file after each iteration\n",
    "        iteration_num = start_idx // batch_size\n",
    "        save_path = f\"{output_folder}/iteration_{iteration_num}_{end_idx}.csv\"\n",
    "        save_to_csv(batch_instructions, batch_outputs, translated_batch_instructions, translated_batch_outputs, save_path)\n",
    "\n",
    "    translated_dataset = {\n",
    "        \"instruction_english\": dataset[\"instruction\"],  \n",
    "        \"output_english\":  dataset[\"output\"],  \n",
    "        \"instruction_kannada\": translated_instructions,\n",
    "        \"output_kannda\": translated_outputs\n",
    "    }\n",
    "\n",
    "    return translated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_dataset = translate_dataset_to_kannada(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(translated_dataset)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"translated_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_dataset_hf = load_dataset(\"csv\",data_files=\"./translated_final.csv\")\n",
    "translated_dataset_hf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_dataset_hf.push_to_hub('CognitiveLab/translation-combined-dataset-final')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
